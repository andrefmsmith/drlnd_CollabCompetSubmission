{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from unityagents import UnityEnvironment\n",
    "from collections import namedtuple, deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Tennis_Windows_x86_64/Tennis.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "wimbledon = env.brain_names[0]\n",
    "brain = env.brains[wimbledon]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[wimbledon]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agents and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agents' performance, if they select actions at random with each time step.  A window should pop up that allows you to observe the agents.\n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agents are able to use their experiences to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (max over agents) from episode 1: 0.0\n",
      "Score (max over agents) from episode 2: 0.0\n",
      "Score (max over agents) from episode 3: 0.0\n",
      "Score (max over agents) from episode 4: 0.09000000171363354\n",
      "Score (max over agents) from episode 5: 0.0\n"
     ]
    }
   ],
   "source": [
    "ep_len = np.zeros((5), dtype='int32')\n",
    "for i in range(1, 6):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[wimbledon]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        ep_len[i-1]+=1\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[wimbledon]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(3e6)     # replay buffer size\n",
    "BATCH_SIZE = 8          # minibatch size\n",
    "GAMMA = 0.99               # discount factor\n",
    "TAU = 1e-3                 # for soft update of target parameters\n",
    "LR_ACTOR = 1e-4            # learning rate of the actor \n",
    "LR_CRITIC = 2e-4           # learning rate of the critic\n",
    "update_every = 20          # number of timesteps after which to run an update\n",
    "SN = 0.25                  # starting value for additive noise scale (exploratory actions)\n",
    "ND = 0.99999               # noise decay rate (exploratory actions)\n",
    "NM = 0.01                  # noise minimum to be maintained (exploratory actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG agents\n",
    "____\n",
    "### Define Actor and Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hu=(400, 300), activ_in = F.relu, activ_out = torch.tanh):\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        self.activ_in = activ_in\n",
    "        self.activ_out = activ_out\n",
    "        \n",
    "        self.input_layer = nn.Linear(state_size, hu[0])\n",
    "        self.hl1 = nn.Linear(hu[0], hu[1])\n",
    "        self.output_layer = nn.Linear(hu[-1], action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        x = self.activ_in(self.input_layer(x))\n",
    "        x = self.activ_in(self.hl1(x))\n",
    "        return self.activ_out(self.output_layer(x))  \n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hu=(400, 300), activ_in = F.relu):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        self.activ_in = activ_in\n",
    "        \n",
    "        self.input_layer = nn.Linear(state_size, hu[0])\n",
    "        self.hl1 = nn.Linear(hu[0]+action_size, hu[1])        \n",
    "        self.output_layer = nn.Linear(hu[-1], 1)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = state\n",
    "        u = action\n",
    "        \n",
    "        x = self.activ_in(self.input_layer(x))\n",
    "        x = torch.cat((x, u), dim=1)\n",
    "        x = self.activ_in(self.hl1(x))\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define DDPG agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG_Agent():\n",
    "    def __init__(self, state_size, action_size, start_noise=SN, noise_decay=ND, noise_min=NM, add_noise=True):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        ### Initialise actor online and target networks\n",
    "        self.actor_online = Actor(state_size, action_size).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_online.parameters(), lr=LR_ACTOR)\n",
    "        \n",
    "        ### Initialise critic online and target networks\n",
    "        self.critic_online = Critic(state_size, action_size).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_online.parameters(), lr=LR_CRITIC)\n",
    "        \n",
    "        \n",
    "        ### Noise parameters for exploration\n",
    "        self.noise_scale = start_noise\n",
    "        self.noise_decay = noise_decay\n",
    "        self.noise_min = noise_min\n",
    "        \n",
    "        self.add_noise = add_noise\n",
    "        \n",
    "        ### Replay buffer\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE)\n",
    "        \n",
    "        ### Keep track of timesteps since last training update\n",
    "        self.update = 0\n",
    "    \n",
    "    ### Generate random normal noise to add to the action output and enable exploration\n",
    "    def generate_noise(self):\n",
    "        noise = np.random.normal(loc=0, scale=self.noise_scale, size=self.action_size)\n",
    "        self.noise_scale = max(self.noise_decay*self.noise_scale, self.noise_min)\n",
    "        return noise\n",
    "    \n",
    "    ### Store experiences in the replay buffer and control training updates\n",
    "    def step(self, state, action, reward, next_state, done, update_cycles=3):\n",
    "        # Commit experience to memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        self.update = (self.update +1)%update_every\n",
    "        \n",
    "        # Run optimisation 'update_cycles' times\n",
    "        c=0\n",
    "        if (self.update==0):\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                while c < update_cycles:\n",
    "                    experiences = self.memory.sample()\n",
    "                    self.learn(experiences, GAMMA)\n",
    "                    c+=1\n",
    "    \n",
    "    ### Use the actor network to select an action. OPTIONAL: add noise to the action (add_noise=True)\n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_online.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_online(state).cpu().data.numpy()\n",
    "        self.actor_online.train()\n",
    "        if self.add_noise:\n",
    "            action += self.generate_noise()\n",
    "        return np.clip(action, -1, 1)\n",
    "    \n",
    "    ### Run a training update from a batch of experiences on the critic and actor online networks and a \n",
    "    ## soft update on the respective target networks\n",
    "    def learn(self, experiences, gamma):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        ### Update critic\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1-dones))\n",
    "        Q_expected = self.critic_online(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        \n",
    "        # Backprop\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic_online.parameters(), 1)\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        \n",
    "        ### Update actor\n",
    "        actions_pred = self.actor_online(states)\n",
    "        actor_loss = -self.critic_online(states, actions_pred).mean()\n",
    "        \n",
    "        # Backprop\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.actor_online.parameters(), 1)\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        \n",
    "        ### Update targets\n",
    "        self.soft_update(self.critic_online, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_online, self.actor_target, TAU)\n",
    "    \n",
    "    ### Blend together online and target network parameters\n",
    "    def soft_update(self, online_model, target_model, tau):\n",
    "        for target_param, online_param in zip(target_model.parameters(), online_model.parameters()):\n",
    "            target_param.data.copy_(tau*online_param.data + (1.0-tau)*target_param.data)\n",
    "            \n",
    "### Define a Replaybuffer class to store experiences, organise and sample from them\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, action_size, buffer_size, batch_size):\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    \n",
    "    ### Adds an experience tuple to memory\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    ### Samples k (batch size) experience tuples randomly from memory\n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train a DDPG agent, monitor score and save Actor/Critic checkpoints if task solved.\n",
    "def MADDPG(output, player1, player2, n_episodes=200, max_t=1000):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=False)[wimbledon]           # reset the environment \n",
    "        state_p1 = env_info.vector_observations[0]                     # get the current state\n",
    "        state_p2 = env_info.vector_observations[1] \n",
    "        score = np.array([0,0], dtype='float64')                                                   # initialize the score\n",
    "        for t in range(max_t):\n",
    "            action_p1 = player1.act(state_p1)                               # select an action\n",
    "            action_p2 = player2.act(state_p2)\n",
    "            combined_a = np.concatenate((action_p1, action_p2))\n",
    "            env_info = env.step(combined_a)[wimbledon]                 # send action to tne environment\n",
    "            next_state_p1 = env_info.vector_observations[0]            # get next state\n",
    "            next_state_p2 = env_info.vector_observations[1]\n",
    "            reward_p1 = env_info.rewards[0]                            # get reward\n",
    "            reward_p2 = env_info.rewards[1]\n",
    "            collab_reward = sum(env_info.rewards)\n",
    "            done = env_info.local_done                           # check if episode finished\n",
    "            player1.step(state_p1, action_p1, reward_p1, next_state_p1, done[0])     # agent takes one step to train\n",
    "            player2.step(state_p2, action_p2, reward_p2, next_state_p2, done[1])\n",
    "            state_p1 = next_state_p1                                      # roll over state to next time step\n",
    "            state_p2 = next_state_p2\n",
    "            score += env_info.rewards                                         # update the score\n",
    "            if done:                                                # exit loop if episode finished\n",
    "                break \n",
    "        scores_deque.append(np.max(score))\n",
    "        scores.append(score)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.3f}\\tScore: {:.3f}'.format(i_episode, np.mean(scores_deque), np.max(score)), end=\"\")\n",
    "        #print('Federer {:.4f}-{:.4f} Murray'.format(scores[-1][0], scores[-1][1]))\n",
    "        if np.mean(scores_deque)>=0.5:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_deque)))\n",
    "            #torch.save(agent.actor_online.state_dict(), '{}_checkpoint_actor.pth'.format(output))\n",
    "            #torch.save(agent.critic_online.state_dict(), '{}_checkpoint_critic.pth'.format(output))\n",
    "            break\n",
    "            \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 2\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 3\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 4\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 5\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 6\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 7\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 8\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 9\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 10\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 11\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 12\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 13\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 14\tAverage Score: 0.000\tScore: 0.000Federer 0.0000--0.0100 Murray\n",
      "Episode 15\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 16\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 17\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 18\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 19\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 20\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 21\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 22\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 23\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 24\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 25\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 26\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 27\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 28\tAverage Score: 0.000\tScore: 0.000Federer 0.0000--0.0100 Murray\n",
      "Episode 29\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 30\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 31\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 32\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 33\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 34\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 35\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 36\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 37\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 38\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 39\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 40\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 41\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 42\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 43\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 44\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 45\tAverage Score: 0.000\tScore: 0.000Federer -0.0100-0.0000 Murray\n",
      "Episode 46\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 47\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 48\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 49\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 50\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 51\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 52\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 53\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 54\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 55\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 56\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 57\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 58\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 59\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 60\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 61\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 62\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 63\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 64\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 65\tAverage Score: 0.000\tScore: 0.000Federer -0.0100-0.0000 Murray\n",
      "Episode 66\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 67\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 68\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 69\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 70\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 71\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 72\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 73\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 74\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 75\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 76\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 77\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 78\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 79\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 80\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 81\tAverage Score: 0.000\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 82\tAverage Score: 0.001\tScore: 0.100Federer 0.0000-0.1000 Murray\n",
      "Episode 83\tAverage Score: 0.001\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 84\tAverage Score: 0.001\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 85\tAverage Score: 0.001\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 86\tAverage Score: 0.001\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 87\tAverage Score: 0.001\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 88\tAverage Score: 0.001\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 89\tAverage Score: 0.001\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 90\tAverage Score: 0.001\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 91\tAverage Score: 0.001\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 92\tAverage Score: 0.001\tScore: 0.000Federer 0.0000--0.0100 Murray\n",
      "Episode 93\tAverage Score: 0.001\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 94\tAverage Score: 0.001\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 95\tAverage Score: 0.001\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 96\tAverage Score: 0.001\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 97\tAverage Score: 0.001\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 98\tAverage Score: 0.001\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 99\tAverage Score: 0.001\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 100\tAverage Score: 0.001\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 101\tAverage Score: 0.001\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 102\tAverage Score: 0.001\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 103\tAverage Score: 0.001\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 104\tAverage Score: 0.001\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 105\tAverage Score: 0.001\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 106\tAverage Score: 0.001\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 107\tAverage Score: 0.001\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 108\tAverage Score: 0.002\tScore: 0.100Federer 0.0000-0.1000 Murray\n",
      "Episode 109\tAverage Score: 0.002\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 110\tAverage Score: 0.002\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 111\tAverage Score: 0.002\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 112\tAverage Score: 0.002\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 113\tAverage Score: 0.002\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 114\tAverage Score: 0.002\tScore: 0.000Federer 0.0000-0.0000 Murray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 115\tAverage Score: 0.002\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 116\tAverage Score: 0.002\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 117\tAverage Score: 0.002\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 118\tAverage Score: 0.002\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 119\tAverage Score: 0.002\tScore: 0.000Federer 0.0000--0.0100 Murray\n",
      "Episode 120\tAverage Score: 0.002\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 121\tAverage Score: 0.002\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 122\tAverage Score: 0.002\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 123\tAverage Score: 0.002\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 124\tAverage Score: 0.002\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 125\tAverage Score: 0.002\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 126\tAverage Score: 0.002\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 127\tAverage Score: 0.002\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 128\tAverage Score: 0.002\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 129\tAverage Score: 0.002\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 130\tAverage Score: 0.002\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 131\tAverage Score: 0.002\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 132\tAverage Score: 0.002\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 133\tAverage Score: 0.002\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 134\tAverage Score: 0.002\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 135\tAverage Score: 0.003\tScore: 0.100Federer 0.0000-0.1000 Murray\n",
      "Episode 136\tAverage Score: 0.003\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 137\tAverage Score: 0.003\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 138\tAverage Score: 0.003\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 139\tAverage Score: 0.003\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 140\tAverage Score: 0.003\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 141\tAverage Score: 0.003\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 142\tAverage Score: 0.003\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 143\tAverage Score: 0.003\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 144\tAverage Score: 0.003\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 145\tAverage Score: 0.003\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 146\tAverage Score: 0.003\tScore: 0.000Federer 0.0000--0.0100 Murray\n",
      "Episode 147\tAverage Score: 0.003\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 148\tAverage Score: 0.003\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 149\tAverage Score: 0.003\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 150\tAverage Score: 0.003\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 151\tAverage Score: 0.003\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 152\tAverage Score: 0.003\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 153\tAverage Score: 0.003\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 154\tAverage Score: 0.003\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 155\tAverage Score: 0.003\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 156\tAverage Score: 0.003\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 157\tAverage Score: 0.003\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 158\tAverage Score: 0.003\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 159\tAverage Score: 0.003\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 160\tAverage Score: 0.003\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 161\tAverage Score: 0.003\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 162\tAverage Score: 0.003\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 163\tAverage Score: 0.004\tScore: 0.100Federer 0.0000-0.1000 Murray\n",
      "Episode 164\tAverage Score: 0.004\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 165\tAverage Score: 0.004\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 166\tAverage Score: 0.004\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 167\tAverage Score: 0.004\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 168\tAverage Score: 0.004\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 169\tAverage Score: 0.004\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 170\tAverage Score: 0.004\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 171\tAverage Score: 0.004\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 172\tAverage Score: 0.004\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 173\tAverage Score: 0.004\tScore: 0.000Federer -0.0100-0.0000 Murray\n",
      "Episode 174\tAverage Score: 0.004\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 175\tAverage Score: 0.004\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 176\tAverage Score: 0.004\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 177\tAverage Score: 0.004\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 178\tAverage Score: 0.004\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 179\tAverage Score: 0.004\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 180\tAverage Score: 0.004\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 181\tAverage Score: 0.004\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 182\tAverage Score: 0.003\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 183\tAverage Score: 0.003\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 184\tAverage Score: 0.003\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 185\tAverage Score: 0.003\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 186\tAverage Score: 0.003\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 187\tAverage Score: 0.003\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 188\tAverage Score: 0.003\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 189\tAverage Score: 0.003\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 190\tAverage Score: 0.003\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 191\tAverage Score: 0.004\tScore: 0.100Federer 0.1000-0.0000 Murray\n",
      "Episode 192\tAverage Score: 0.004\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 193\tAverage Score: 0.004\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 194\tAverage Score: 0.004\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 195\tAverage Score: 0.004\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 196\tAverage Score: 0.004\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 197\tAverage Score: 0.004\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 198\tAverage Score: 0.004\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 199\tAverage Score: 0.004\tScore: 0.000Federer 0.0000-0.0000 Murray\n",
      "Episode 200\tAverage Score: 0.004\tScore: 0.000Federer 0.0000-0.0000 Murray\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "federer = DDPG_Agent(state_size, action_size)\n",
    "murray = DDPG_Agent(state_size, action_size)\n",
    "\n",
    "scores = MADDPG(test, federer, murray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "testscores = []\n",
    "testscore = np.array([5,0])\n",
    "testdeque = deque(maxlen=100)\n",
    "testdeque.append(np.max(testscore))\n",
    "testscores.append(testscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([5, 0])]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([5])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdeque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([5, 0])]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testscores[-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(testscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

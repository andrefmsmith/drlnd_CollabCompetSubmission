{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRLND Project 3 Report: Collaboration and Competition\n",
    "______________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This is my submission report for Project 3 (Collaboration & Competition) of Udacity's Deep Reinforcement Learning Nanodegree.  \n",
    "\n",
    "In this project, we train 2 agents to play against each other in the Udacity Tennis environment. Agents are rewarded for bouncing the ball over the net and punished for knocking it out of bounds or letting it hit the ground. Their overall goal therefore is to keep bouncing the ball to each other for as long as possible. The task has a larger element of collaboration than competition since the only way agents can maximize their score is if their action allows the other agent to hit the ball back, keeping it within grounds. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background to Actor-Critic & DDPG\n",
    "For my submission, I used DDPG. I covered the background for this algorithm in [Project 2](https://github.com/andrefmsmith/drlnd_ContinuousCtrlSubmission/blob/master/Report.ipynb).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG Algorithm\n",
    "### Overview of my approach\n",
    "For this assignment, I adapted my implementation of DDPG from the [Continuous Control Project](https://github.com/andrefmsmith/drlnd_ContinuousCtrlSubmission). A step by step description of the general DDPG algorithm has been presented in the [Algorithm overview](https://github.com/andrefmsmith/drlnd_ContinuousCtrlSubmission/blob/master/Report.ipynb) cell block of the Continuous Control Project report.  \n",
    "\n",
    "For this assignment, I have a single Actor-Critic instance controlling the two agents. The **Actor network** takes current state as input and outputs an action vector, and the **Critic network** takes in current state at the input layer and current action at the first hidden layer, outputting a single action value given current state and action.   \n",
    "\n",
    "Apart from simplicity, my reasoning for approaching the task with a single Actor-Critic is that both agents are observing similar states and working within the boundaries of the same action space. During task performance, at each timestep the network does a forward pass operation on each of the agents' states and returns an action for each to perform.  \n",
    "\n",
    "Because of the similarity in state and action spaces, there should be a great degree of generalization occurring in mapping observed states to performed actions between the two rackets.In this environment, I imagine that all of the state and action variables are either:\n",
    "- Mirrored between agents, ie the net is coordinate 0 and one racket's position is for example +5.0 whereas the other's is -5.0, or\n",
    "- The same between agents, ie the variables reflect absolute distance to the net (+5.0 in both cases above).  \n",
    "\n",
    "In the first case, the transform from one agent's state and action space to the others is trivial (a sign inversion) and therefore easy to learn, whereas in the second there's not even any transform needed. In both cases it seems to me we don't necessarily need to learn two policies and have distinct Actor networks controlling the agents and distinct Critic networks evaluating their actions, but instead that a single, wide-enough Actor-Critic should suffice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor and Critic architectures\n",
    "The **Actor** is a state-in-action-out network therefore its input layer is sized (state_size, hu[0]), and its output layer is sized (hu[-1], action_size), where hu is a tuple defining the number of hidden units. I went with hu=(256, 128) to ensure the network was wide enough to learn any transforms needed on the state space. The activation function for inner layers was ReLu, whereas for the output layer it was tanh since the action space boundaries were [-1, 1] for both variables. Had it been different, I would have needed to write an appropriate scaling function.  \n",
    "\n",
    "Finally, for this task I chose to try out Batch Normalization layers, given that trajectories could be quite different.\n",
    "```Python\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hu=(256, 128),\n",
    "                 activ_in = F.relu, activ_out = torch.tanh):\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        self.activ_in = activ_in\n",
    "        self.activ_out = activ_out\n",
    "        \n",
    "        self.input_layer = nn.Linear(state_size, hu[0])\n",
    "        self.bn1 = nn.BatchNorm1d(hu[0])\n",
    "        self.hl1 = nn.Linear(hu[0], hu[1])\n",
    "        self.bn2 = nn.BatchNorm1d(hu[1])\n",
    "        self.output_layer = nn.Linear(hu[-1], action_size)\n",
    "        self.bn3 = nn.BatchNorm1d(action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        x = self.activ_in(self.input_layer(x))\n",
    "        x = self.bn1(x)\n",
    "        x = self.activ_in(self.hl1(x))\n",
    "        x = self.bn2(x)\n",
    "        x = self.output_layer(x)\n",
    "        x = self.bn3(x)\n",
    "        return self.activ_out(x)```  \n",
    "        \n",
    "The **Critic** is a state-in-value-out network where its input layer is sized (state_size, hu[0]). The agent's current action is fed in at the first hidden layer by concatenating it to the output of the input layer. The first hidden layer size is therefore (hu[0]+action_size, hu[1]) with the output layer being sized (hu[-1], 1) so that it outputs a single Q value. As done for the Actor, I'm taking advantage of Batch Normalization here as well. The Critic's output is a single float for Q value therefore no activation function is needed:\n",
    "```Python\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hu=(256, 128), activ_in = F.leaky_relu):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        self.activ_in = activ_in\n",
    "        \n",
    "        self.bn0 = nn.BatchNorm1d(state_size)\n",
    "        self.input_layer = nn.Linear(state_size, hu[0])\n",
    "        self.bn1 = nn.BatchNorm1d(hu[0])\n",
    "        self.hl1 = nn.Linear(hu[0]+action_size, hu[1])\n",
    "        self.bn2 = nn.BatchNorm1d(hu[1])\n",
    "        self.output_layer = nn.Linear(hu[-1], 1)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        state = self.bn0(state)\n",
    "        xs = self.input_layer(state)\n",
    "        xs = self.bn1(xs)\n",
    "        xs = self.activ_in(xs)\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = self.hl1(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.activ_in(x)\n",
    "        return self.output_layer(x)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDPG agent Attributes and Methods\n",
    "I used essentially the same attributes and methods as in the Continuous Control submission. Briefly, the DDPG agent is specified with a number of attributes useful to have easily accessible and then instantiates Online and Target networks for both Actor and Critic, specifying different optimizers for each. This is useful because it works to our advantage to have the Critic learn faster since its output values are used to train the Actor.  \n",
    "\n",
    "```Python\n",
    "class DDPG_agent():\n",
    "    def __init__(self, state_size, action_size, num_agents=2, batch_size=BATCH_SIZE,\n",
    "                 start_noise=SN, noise_decay=ND, noise_min=NM, add_noise=True, update_cycles=UC):\n",
    "        super(DDPG_agent, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.num_agents = num_agents\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        ### Initialise actor online and target networks\n",
    "        self.actor_online = Actor(state_size, action_size).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_online.parameters(), lr=LR_ACTOR)\n",
    "        \n",
    "        ### Initialise critic online and target networks\n",
    "        self.critic_online = Critic(state_size, action_size).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_online.parameters(), lr=LR_CRITIC)```\n",
    "\n",
    "In this assignment I also wrote a function to ensure target and online networks were initialised with the same weights exactly, which sounded like a reasonable idea:\n",
    "```Python\n",
    "def equalize_OnlineTarget(self, target, online):\n",
    "    for target_param, online_param in zip(target.parameters(), online.parameters()):\n",
    "        target_param.data.copy_(online_param.data)```\n",
    "\n",
    "My strategy for exploratory behaviour involves simply adding noise drawn from a numpy normal distribution centered at 0 and with standard deviation ```noise_scale``` to the agent's action output, using the method ```generate_noise```. Therefore, the agent is initialised with a certain ```noise_scale```, which is updated by multiplying by a ```noise_decay``` every time the agent takes an action. A noise floor, ```noise_min``` is specified to ensure exploratory behaviour continues throughout the task.  \n",
    "\n",
    "```Python\n",
    "### Agent's internal noise-related attributes\n",
    "        self.noise_scale = start_noise\n",
    "        self.noise_decay = noise_decay\n",
    "        self.noise_min = noise_min\n",
    "        self.add_noise = add_noise\n",
    "### Agent's method for generating noise\n",
    "        def generate_noise(self):\n",
    "            noise = np.random.normal(loc=0, scale=self.noise_scale, size=self.action_size)\n",
    "            self.noise_scale = max(self.noise_decay*self.noise_scale, self.noise_min)\n",
    "            return noise```\n",
    "\n",
    "The agent keeps track of how long it's been since it ran an update with the attribute ```self.update``` and whether enough timesteps have passed to run a new one with hyperparameter ```update_cycles```. As before, the agent has a ```self.memory``` attribute which is an instatiation of the ```ReplayBuffer``` class:\n",
    "\n",
    "```Python\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, action_size, buffer_size, batch_size):\n",
    "        self.action_size = action_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.memory = deque(maxlen=self.buffer_size)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done, num_agents):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        for i in range(num_agents):\n",
    "            e = self.experience(state[i], action[i], reward[i], next_state[i], done[i])\n",
    "            self.memory.append(e)      \n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)```\n",
    "        \n",
    "The only difference to this version being that during execution of the method ```add```, we unpack and add the experiences of each individual agent to memory. This means that later, when we call the method ```learn``` we draw on experiences from both agents to update our networks.  \n",
    "```Python\n",
    "def learn(self, experiences, gamma):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        \n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        \n",
    "        Q_expected = self.critic_online(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic_online.parameters(), 1)\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        \n",
    "        actions_pred = self.actor_online(states)\n",
    "        actor_loss = -self.critic_online(states, actions_pred).mean()\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        self.soft_update(self.critic_online, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_online, self.actor_target, TAU)```\n",
    "        \n",
    "```learn``` takes in a batch of experiences which is sampled randomly by the ```sample``` method of the Replay Buffer class. It queries the target networks first for the action to perform in the next states, and then for the value of these actions, discounting them and adding observed reward in order to return an estimate of the value for the current action given the current state. Then it computes the online Critic's estimate of the current state-action value and the loss, before backpropagation and weight updating.  \n",
    "\n",
    "To update the Actor, we first run a forward pass through the online Actor network of the batch's states, obtaining the current policy's proposed action. Our 'Loss' is calculated by passing these actions and the states in the batch through the online critic network, which returns their value. If we take its negative, we can perform gradient ascent, thus updating weights in the Actor online network to maximise the likelihood of outputting actions of high value. Finally, both target networks are updated via a weighted average between their parameters and the current online networks'. This equates not to freezing/fixing targets but moving them very slowly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "```Python\n",
    "BUFFER_SIZE = int(1e6)     # replay buffer size\n",
    "BATCH_SIZE = 1024            # minibatch size\n",
    "GAMMA = 0.99               # discount factor\n",
    "TAU = 1e-3                 # for soft update of target parameters\n",
    "LR_ACTOR = 3e-4            # learning rate of the actor \n",
    "LR_CRITIC = 3e-3           # learning rate of the critic\n",
    "update_every = 20           # number of timesteps after which to run an update\n",
    "SN = 0.5                   # starting value for additive noise scale (exploratory actions)\n",
    "ND = 0.999                 # noise decay rate (exploratory actions)\n",
    "NM = 0.1                  # noise minimum to be maintained (exploratory actions)\n",
    "UC = 10                     # Number of cycles to run updates for```\n",
    "\n",
    "I found it beneficial to work with fairly large batch sizes and a higher learning rate, especially for the Critic network. The latter is particularly useful since the sooner we learn how to infer value well, the earlier we start generating better actions. I kept a fairly high minimum noise added for exploratory actions, and used a high (10) number of update cycles to run per batch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "Using this implementation, the task was solved in around 1,000 episodes. During training, I increased the criterion to an average score of 0.75 since I wanted to watch agents playing well. \n",
    "![plot](Solved.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for future research\n",
    "- Double Q learning: in DDQN, we use a double network for estimating targets in order to solve the problem of 'optimism' in action values.\n",
    "- Prioritized replay: instead of sampling experiences at random, do so based on how much there is to be learnt from them. This would involve a different loss function, since prioritised experiences would need to be weighted in proportion to how (in)frequent they are so as not to train on a different distribution from the one actually observed by the agent.\n",
    "- A 'real' multi-agent implementation: as discussed above, my reasoning for having a single Actor-Critic structure controlling both agents was principled on the structure, generalization desirability and collaborative nature of the task. I'd be curious to implement 2 separate policy networks to see if different strategies unfolded for the 2 agents. One way to do this would perhaps be to implement 2 critics where each has access to its states plus the two agents' actions. I think a shared critic in this situation would result in very similar policies developing, hence my preference for distinct critics, which would also entail each agent having its own replay buffer. The idea would be that competition leads to each agent pulling the other's performance up in an 'arms race', as an alternative to the current more collaborative approach. I don't know if this would work, but I like the 'realism' of an agent being able to observe only its own states and the opponent's actions.\n",
    "- An interesting thing I noticed in my solution and others' is that performance seems to follow a slow time-scale of peaks and valleys during learning. For example, there's a bump in performance between episode 200 and 500, and a valley from there to around 650. This is followed by a bump into a plateau and a higher bump shortly before 1,000 episodes which itself is followed by a drop and a further increase. I wonder where this behaviour comes from. It can't be the exploration rate contributed by additive noise because that is monotonically declining. Beyond some residual inherent instability of deep nets in function approximation, I don't have a particular idea where these slow dynamics come from, but if the reviewer has a suggestion or literature which has dug into it, I'd be really interested to know more.\n",
    "\n",
    "Thanks for reading!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
